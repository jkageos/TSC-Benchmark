# TSC-Benchmark Configuration - SCIENTIFICALLY FAIR COMPARISON

# Execution mode and behavior
execution:
  mode: "benchmark" # Options: benchmark, test, tune, single
  verbose: true
  # Test mode settings
  test_model: "fcn"
  test_dataset: "Beef"
  # Tune mode settings
  tune_rounds: 2
  # Single mode settings
  single_model: "fcn"
  single_dataset: "CinCECGTorso"

# Target datasets for benchmarking
datasets:
  - "Adiac"
  - "ArrowHead"
  - "Beef"
  - "Car"
  - "ChlorineConcentration"
  - "CinCECGTorso"
  - "FiftyWords"
  - "HouseTwenty"
  - "KeplerLightCurves"

# Hardware and system resource management
hardware:
  device: "cuda"
  use_compile: true
  compile_mode: "reduce-overhead"
  use_amp: true
  max_cpu_load: 0.6
  reserve_cores: 2
  max_workers_override: 4
  auto_workers: true

# Training hyperparameters - CONSISTENT ACROSS ALL MODELS
training:
  epochs: 100
  batch_size: 32 # Conservative for stability
  learning_rate: 0.001
  optimizer: "adamw"
  optimizer_params:
    weight_decay: 0.01
  patience: 15
  normalize: true
  padding: "zero"
  max_length: null # Let datasets use natural length (CRITICAL for fair comparison)
  num_workers: 0
  use_scheduler: true
  warmup_epochs: 5
  use_augmentation: false # CRITICAL: Disabled for fair comparison
  use_tta: false # CRITICAL: No test-time augmentation
  use_swa: false
  swa_start: 60
  cv_folds: 5

# Model architectures - PARAMETER-MATCHED FOR FAIR COMPARISON
models:
  # FCN: ~150K parameters
  fcn:
    type: "fcn"
    hidden_dims: [256, 128]
    dropout_rate: 0.1 # MATCHED across all models
    use_batch_norm: true

  # CNN: ~180K parameters
  cnn:
    type: "cnn"
    num_filters: [64, 128, 256]
    kernel_size: 3
    dropout_rate: 0.1 # MATCHED to CATS

  # Transformer: ~185K parameters (matched to CNN)
  transformer:
    type: "transformer"
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512 # 4*d_model
    dropout: 0.1 # MATCHED to CNN
    factor: 1
    max_seq_len: 5000

  # CATS: ~180K parameters (matched to CNN)
  cats:
    type: "cats"
    d_model: 128 # REDUCED from 192 for parameter matching
    num_heads: 4 # EXPLICIT
    num_layers: 2 # EXPLICIT
    d_ff: 512 # REDUCED from 768 (4*d_model standard)
    dropout: 0.1 # MATCHED to CNN
    max_seq_len: 5000

  # Autoformer: ~170K parameters
  autoformer:
    type: "autoformer"
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512
    dropout: 0.1
    factor: 1
    max_seq_len: 5000

  # PatchTST: ~190K parameters
  patchtst:
    type: "patchtst"
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512
    dropout: 0.1
    patch_len: 16
    stride: 8

# Results and logging
results:
  save_dir: "results"
  save_checkpoints: false

# Dataset-specific overrides - ONLY FOR MEMORY CONSTRAINTS
# NO model-specific capacity changes to avoid bias
dataset_overrides:
  # Small datasets - reduce batch size only
  Beef:
    batch_size: 16 # Small dataset (30 samples)
    num_workers: 0

  ArrowHead:
    batch_size: 16 # Small dataset (36 samples)
    num_workers: 0

  Car:
    batch_size: 16 # Small dataset
    num_workers: 0

  HouseTwenty:
    batch_size: 16 # Small dataset (40 samples)
    num_workers: 0

  # Long sequences - reduce batch size for memory only
  CinCECGTorso:
    batch_size: 16 # Long sequences (1639 timesteps)
    num_workers: 0

  ChlorineConcentration:
    batch_size: 16 # Long sequences (166 timesteps)
    num_workers: 0

  # Medium datasets - standard settings
  Adiac:
    batch_size: 32
    num_workers: 0

  FiftyWords:
    batch_size: 32
    num_workers: 0

  KeplerLightCurves:
    batch_size: 32
    num_workers: 0

# Reproducibility
seed: 42
