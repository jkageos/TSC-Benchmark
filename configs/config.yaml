# TSC-Benchmark Configuration

# Execution mode and behavior
execution:
  mode: "benchmark" # Options: benchmark, test, tune, single
  verbose: true
  # Test mode settings
  test_model: "fcn"
  test_dataset: "Beef"
  # Tune mode settings
  tune_rounds: 2
  # Single mode settings
  single_model: "fcn"
  single_dataset: "CinCECGTorso"

# Target datasets for benchmarking
datasets:
  - "Adiac"
  - "ArrowHead"
  - "Beef"
  - "Car"
  - "ChlorineConcentration"
  - "CinCECGTorso"
  - "FiftyWords"
  - "HouseTwenty"
  - "KeplerLightCurves"

# Hardware and system resource management
hardware:
  device: "cuda"
  use_compile: true # Changed from false
  compile_mode: "reduce-overhead" # Better for inference-heavy benches
  use_amp: true # Keep enabled
  max_cpu_load: 0.6
  reserve_cores: 2
  max_workers_override: 4
  auto_workers: true

# Training hyperparameters
training:
  epochs: 50 # Reduced from 100
  batch_size: 64 # Increased from 48 (use smaller per-dataset if needed)
  learning_rate: 0.001
  optimizer: "adamw"
  optimizer_params:
    weight_decay: 0.01
  patience: 8 # Reduced from 15
  normalize: true
  padding: "zero"
  max_length: 512 # Changed from null (adaptive)
  # This truncates very long sequences early
  num_workers: 0
  use_scheduler: true
  warmup_epochs: 3 # Reduced from 5
  use_augmentation: false # CRITICAL: Disabled for fair comparison
  use_tta: false # CRITICAL: No test-time augmentation
  tta_augmentations: 5
  use_swa: false
  swa_start: 60
  cv_folds: 3 # Reduced from 5 (still statistically sound)

# Model architectures - Memory-optimized
models:
  fcn:
    type: "fcn"
    hidden_dims: [256, 128]
    dropout: 0.3
    use_batch_norm: true

  cnn:
    type: "cnn"
    num_filters: [64, 128, 256]
    kernel_size: 5
    dropout_rate: 0.3

  transformer:
    type: "transformer"
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512
    dropout: 0.1
    factor: 1
    max_seq_len: 5000

  cats:
    d_ff: 768
    d_model: 192
    dropout: 0.1
    max_seq_len: 5000

  autoformer:
    type: "autoformer"
    d_model: 96 # Reduced from 128
    num_heads: 3 # Reduced from 4
    num_layers: 1 # Reduced from 2
    d_ff: 384 # Reduced from 512
    dropout: 0.1
    factor: 1
    max_seq_len: 5000

  patchtst:
    type: "patchtst"
    d_model: 96 # Reduced
    num_heads: 3 # Reduced
    num_layers: 1 # Reduced
    d_ff: 384 # Reduced
    dropout: 0.1
    patch_len: 16
    stride: 8

# Results and logging
results:
  save_dir: "results"
  save_checkpoints: true
  log_interval: 10

# Dataset-specific overrides - Aggressive memory management
dataset_overrides:
  Beef:
    epochs: 80
    batch_size: 16 # Small dataset needs small batches
    patience: 10
    num_workers: 0
  Car:
    epochs: 80
    batch_size: 16
    patience: 10
    num_workers: 0
  ChlorineConcentration:
    batch_size: 32 # Medium dataset
    max_length: 128 # Keep aggressive
    num_workers: 0
  FiftyWords:
    epochs: 120
    patience: 20
    batch_size: 16 # Reduced from 32
    max_length: 256
    num_workers: 0
  KeplerLightCurves:
    batch_size: 16 # Reduced from 24
    max_length: 256 # Increase slightly from 128 (helps accuracy)
    epochs: 60
    patience: 10
    num_workers: 0
  Adiac:
    batch_size: 16 # Reduced from 32
    max_length: 256 # More lenient than 256 global
    num_workers: 0
  CinCECGTorso:
    batch_size: 16 # Reduced from 32
    max_length: 512 # Increase to preserve signal
    num_workers: 0
  ArrowHead:
    batch_size: 32
    num_workers: 0
  HouseTwenty:
    batch_size: 16
    epochs: 100
    patience: 15
    num_workers: 0

# Reproducibility
seed: 42
