# TSC-Benchmark Configuration with Multiple Transformer Variants

# Target datasets for benchmarking
datasets:
  - Adiac
  - ArrowHead
  - Beef
  - Car
  - ChlorineConcentration
  - CinCECGTorso
  - FiftyWords
  - HouseTwenty
  - KeplerLightCurves

# Model configurations
models:
  fcn:
    hidden_dims: [512, 256, 128]
    dropout_rate: 0.3
    use_batch_norm: true
  
  cnn:
    num_filters: [64, 128, 256]
    kernel_size: 3
    dropout_rate: 0.3
  
  transformer:
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512
    dropout: 0.1
    max_seq_len: 5000
  
  cats:
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512
    dropout: 0.1
    max_seq_len: 5000
  
  autoformer:
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512
    dropout: 0.1
    factor: 1
    max_seq_len: 5000
  
  patchtst:
    d_model: 128
    num_heads: 4
    num_layers: 2
    d_ff: 512
    dropout: 0.1
    patch_len: 16
    stride: 8

# Training configuration
training:
  epochs: 150
  batch_size: 32
  learning_rate: 0.0005
  patience: 20
  optimizer: adamw
  optimizer_params:
    weight_decay: 0.0001
    betas: [0.9, 0.999]
  
  # Learning rate scheduler
  use_scheduler: true
  
  # Device - set to cuda for GPU, auto-detects if null
  device: cuda
  
  # Data preprocessing
  normalize: true
  padding: none  # options: none, zero, repeat
  max_length: null  # null for no padding/truncation

# Results tracking
results:
  output_dir: results
  save_checkpoints: true
  save_history: true
  
# Reproducibility
seed: 42