dataset_overrides:
  Adiac:
    batch_size: 16
    epochs: 60
    max_length: 512
    models:
      autoformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      cats:
        d_ff: 1024
        d_model: 256
        dropout: 0.05
        num_heads: 4
        num_layers: 3
      patchtst:
        d_ff: 768
        d_model: 192
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      transformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.05
        num_heads: 4
        num_layers: 3
    patience: 8
  ArrowHead:
    batch_size: 32
    epochs: 80
    models:
      autoformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      cats:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
      patchtst:
        d_ff: 768
        d_model: 192
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      transformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
    patience: 12
  Beef:
    batch_size: 8
    epochs: 80
    models:
      autoformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      cats:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
      cnn:
        dropout_rate: 0.1
        kernel_size: 5
        num_filters:
        - 32
        - 64
        - 128
      patchtst:
        d_ff: 768
        d_model: 192
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      transformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
    patience: 10
  Car:
    batch_size: 8
    epochs: 80
    models:
      autoformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      cats:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
      fcn:
        dropout_rate: 0.25
        hidden_dims:
        - 512
        - 256
      patchtst:
        d_ff: 768
        d_model: 192
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      transformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
    patience: 10
  ChlorineConcentration:
    batch_size: 8
    epochs: 60
    max_length: 256
    models:
      autoformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      cats:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
      patchtst:
        d_ff: 768
        d_model: 192
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      transformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
    patience: 8
  CinCECGTorso:
    batch_size: 16
    epochs: 60
    models:
      autoformer:
        d_ff: 512
        d_model: 128
        dropout: 0.15
        num_heads: 4
        num_layers: 2
    patience: 8
  FiftyWords:
    batch_size: 32
    epochs: 80
    models:
      autoformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      cats:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
      patchtst:
        d_ff: 768
        d_model: 192
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      transformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
    patience: 12
  HouseTwenty:
    batch_size: 32
    epochs: 60
    models:
      autoformer:
        d_ff: 512
        d_model: 128
        dropout: 0.15
        num_heads: 4
        num_layers: 2
      cats:
        d_ff: 512
        d_model: 128
        dropout: 0.15
        num_heads: 4
        num_layers: 2
      transformer:
        d_ff: 512
        d_model: 128
        dropout: 0.15
        num_heads: 4
        num_layers: 2
    patience: 8
  KeplerLightCurves:
    batch_size: 16
    epochs: 50
    max_length: 256
    models:
      autoformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      cats:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
      cnn:
        dropout_rate: 0.3
        num_filters:
        - 128
        - 256
        - 512
      fcn:
        dropout_rate: 0.1
      patchtst:
        d_ff: 768
        d_model: 192
        dropout: 0.1
        num_heads: 4
        num_layers: 3
      transformer:
        d_ff: 1024
        d_model: 256
        dropout: 0.08
        num_heads: 4
        num_layers: 3
    patience: 10
datasets:
- Adiac
- ArrowHead
- Beef
- Car
- ChlorineConcentration
- CinCECGTorso
- FiftyWords
- HouseTwenty
- KeplerLightCurves
hardware:
  compile_mode: default
  device: cuda
  use_torch_compile: true
models:
  autoformer:
    d_ff: 768
    d_model: 192
    dropout: 0.1
    factor: 1
    max_seq_len: 5000
    num_heads: 4
    num_layers: 3
  cats:
    d_ff: 768
    d_model: 192
    dropout: 0.1
    max_seq_len: 5000
    num_heads: 4
    num_layers: 3
  cnn:
    dropout_rate: 0.2
    kernel_size: 3
    num_filters:
    - 64
    - 128
    - 256
  fcn:
    dropout_rate: 0.2
    hidden_dims:
    - 512
    - 256
    - 128
    use_batch_norm: true
  patchtst:
    d_ff: 640
    d_model: 160
    dropout: 0.1
    num_heads: 4
    num_layers: 3
    patch_len: 16
    stride: 8
  transformer:
    d_ff: 768
    d_model: 192
    dropout: 0.1
    max_seq_len: 5000
    num_heads: 4
    num_layers: 3
results:
  output_dir: results
  save_checkpoints: false
  save_history: true
seed: 42
training:
  augmentation_params:
    jitter_strength: 0.05
    magnitude_warp_strength: 0.15
    prob: 0.8
    scale_range:
    - 0.9
    - 1.1
  batch_size: 64
  cv_folds: 3
  epochs: 100
  learning_rate: 0.0005
  max_length: null
  normalize: true
  num_workers: 2
  optimizer: adamw
  optimizer_params:
    betas:
    - 0.9
    - 0.95
    weight_decay: 1.0e-05
  padding: repeat
  patience: 15
  swa_start: 60
  tta_augmentations: 10
  use_amp: true
  use_augmentation: true
  use_compile: true
  use_scheduler: true
  use_swa: false
  use_tta: false
  warmup_epochs: 10
